{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion NMIST 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yjKpaa3AbcJE",
        "guJzPQgabk3r"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaodwing/nmist-fashion/blob/Ajout-learning-rate-schedul/Fashion_NMIST_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycgJ9Q0OT0hF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e44742d-10c8-46e8-9d62-a4cf2ab806b6"
      },
      "source": [
        "#Used Joseph Redmon on tiny darknet to produce my work\n",
        "#Used the work of Adrian Rosebrock for the learning rate schedule\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import utils\n",
        "from keras.callbacks import Callback, LearningRateScheduler\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.layers import Input, Dense, BatchNormalization, Conv2D, GlobalAveragePooling2D, Activation, LeakyReLU, MaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report\n",
        "from clr_callback import *\n",
        "from learningratefinder import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcktjWXYbqD7",
        "colab_type": "text"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOTeN7Hzbpv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2019)\n",
        "tf.set_random_seed(2019)\n",
        "\n",
        "(X_train, Y_train ), (X_test, Y_test ) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IdADgEFvBnU",
        "colab_type": "text"
      },
      "source": [
        "#Tiny dark net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dUhDCpxu_pO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c47ec889-745f-4573-dfdc-f96be78d398a"
      },
      "source": [
        "batch_size = 256\n",
        "num_classes = 10 \n",
        "epochs = 48\n",
        "img_cols = X_train.shape[1]\n",
        "img_rows = X_train.shape[2]\n",
        "\n",
        "# CLR hyper-parameters\n",
        "baseLr = 1e-3 #1e-10\n",
        "maxLr = 2e-1 #1e+1\n",
        "stepSize = 8\n",
        "clrMethod = \"triangular\"\n",
        "\n",
        "# name file\n",
        "lrfind_plot_path = os.path.sep.join([\"/content/output\", \"lrfind_plot.png\"])\n",
        "training_plot_path = os.path.sep.join([\"/content/output\", \"training_plot.png\"])\n",
        "clr_plot_path = os.path.sep.join([\"/content/output\", \"clr_plot.png\"])\n",
        "\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols);\n",
        "  X_test  = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols);\n",
        "  input_shape = (1, img_rows, img_cols);\n",
        "else: \n",
        "  X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1);\n",
        "  X_test  = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1);\n",
        "  input_shape = (img_rows, img_cols, 1);\n",
        "\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test  = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test  /= 255\n",
        "\n",
        "Y_train = utils.to_categorical(Y_train, num_classes)\n",
        "Y_test  = utils.to_categorical(Y_test, num_classes)\n",
        "Y_label = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QBRL4_QRR4b",
        "colab_type": "text"
      },
      "source": [
        "#Class used for learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THdH4SCScc1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LearningRateDecay:\n",
        "\tdef plot(self, epochs, title=\"Learning Rate Schedule\"):\n",
        "\t\t# compute the set of learning rates for each corresponding\n",
        "\t\t# epoch\n",
        "\t\t#lrs = [self(i) for i in epochs]\n",
        "\t\ttempList = []\n",
        "\t\tepochs = np.arange(1,epochs)\n",
        "\t\tfor i in epochs:\n",
        "\t\t\ttempList.append(self(i))\n",
        "\t\tlrs = np.array(tempList)\n",
        "\n",
        "\t\t# the learning rate schedule\n",
        "\t\tplt.style.use(\"ggplot\")\n",
        "\t\tplt.figure()\n",
        "\t\tplt.plot(epochs, lrs)\n",
        "\t\tplt.title(title)\n",
        "\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\tplt.ylabel(\"Learning Rate\")\n",
        "\n",
        "\n",
        "class StepDecay(LearningRateDecay):\n",
        "\tdef __init__(self, initAlpha=0.01, factor=0.25, dropEvery=10):\n",
        "\t\t# store the base initial learning rate, drop factor, and\n",
        "\t\t# epochs to drop every\n",
        "\t\tself.initAlpha = initAlpha\n",
        "\t\tself.factor = factor\n",
        "\t\tself.dropEvery = dropEvery\n",
        "\t\n",
        "\tdef __call__(self, epoch):\n",
        "\t\t# compute the learning rate for the current epoch\n",
        "\t\texp = np.floor((1 + epoch) / self.dropEvery)\n",
        "\t\talpha = self.initAlpha * (self.factor ** exp)\n",
        "\t\t\n",
        "\t\t# return the learning rate\n",
        "\t\treturn float(alpha)\n",
        "\t\n",
        "\n",
        "class PolynomialDecay(LearningRateDecay):\n",
        "\tdef __init__(self, maxEpochs=100, initAlpha=0.01, power=1.0):\n",
        "\t\t# store the maximum number of epochs, base learning rate,\n",
        "\t\t# and power of the polynomial\n",
        "\t\t# power=1.0 = linear rate decay\n",
        "\t\tself.maxEpochs = maxEpochs\n",
        "\t\tself.initAlpha = initAlpha\n",
        "\t\tself.power = power\n",
        "\n",
        "\tdef __call__(self, epoch):\n",
        "\t\t# compute the new learning rate based on polynomial decay\n",
        "\t\tdecay = (1 - (epoch / float(self.maxEpochs))) ** self.power\n",
        "\t\talpha = self.initAlpha * decay\n",
        "\n",
        "\t\t# return the new learning rate\n",
        "\t\treturn float(alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcqhDcGwRa5T",
        "colab_type": "text"
      },
      "source": [
        "#Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbjuTgDpxN07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layerConv(output, filters, size, stride, pad, batch_normalization=True, activation='LeakyReLU'):\n",
        "  output = Conv2D(kernel_size = (size,size), filters = filters, strides=stride, padding=pad)(output)\n",
        "  if batch_normalization:\n",
        "    output = BatchNormalization()(output);\n",
        "  \n",
        "  #Activation layer\n",
        "  if activation=='LeakyReLU':\n",
        "    output = LeakyReLU(alpha = 0.1)(output)\n",
        "  else:\n",
        "    output = Activation(activation)(output)\n",
        "  \n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YunSxL6Rdby",
        "colab_type": "text"
      },
      "source": [
        "#Tensor model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp7tlDl2vysT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d39b56c-5263-4a6c-8531-4cedc72234fc"
      },
      "source": [
        "#LAYERS\n",
        "#First set of convolution (9-12)\n",
        "input_img = Input(shape=input_shape)\n",
        "\n",
        "output = layerConv(output=input_img, filters=32, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=8 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=64, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=8 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=64, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "\n",
        "#Max pooling to get 14x14 feature (13)\n",
        "output = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(output)\n",
        "\n",
        "output = layerConv(output=output   , filters=16, size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=64, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=16, size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=64, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=16, size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "\n",
        "\"\"\"\n",
        "output = layerConv(output=input_img, filters=128, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=32 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=32 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "\n",
        "#Max pooling to get 14x14 feature (13)\n",
        "output = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(output)\n",
        "\n",
        "#Second set of convolution (14-19)\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output, filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #512\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output, filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #512\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #128\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#Last layer to get an output of 10 class (19+)\n",
        "output = layerConv(output=output, filters=10, size=1, batch_normalization=False, stride=1, pad=\"same\", activation='linear')\n",
        "output = GlobalAveragePooling2D()(output)\n",
        "output = Activation('softmax')(output)\n",
        "\n",
        "model = Model(input_img, output)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "#Learning Rate schedule\n",
        "LearningRateSchedule = CyclicLR( mode=clrMethod,\n",
        "                                base_lr=baseLr,\n",
        "                                max_lr=maxLr,\n",
        "                                step_size= stepSize * (X_train.shape[0] // batch_size))\n",
        "callbacks = []\n",
        "callbacks.append(LearningRateSchedule)\n",
        "\n",
        "opt = SGD(lr=baseLr, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "aug = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 8)         264       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 28, 28, 8)         32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 64)        4672      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 8)         520       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 8)         32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 28, 28, 64)        4672      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 16)        1040      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 14, 14, 16)        64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 64)        9280      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 14, 14, 16)        1040      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 14, 14, 16)        64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 14, 14, 64)        9280      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 14, 14, 16)        1040      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 14, 14, 16)        64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 14, 14, 10)        170       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 14, 14, 10)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 33,706\n",
            "Trainable params: 33,002\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBSLbvUtmaTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "findBestLR = False\n",
        "if findBestLR:\n",
        "  \n",
        "  # Search for the best learning rate\n",
        "  # initialize the learning rate finder and then train with learning\n",
        "  # rates ranging from 1e-10 to 1e+1\n",
        "  print(\"[INFO] finding learning rate...\")\n",
        "  lrf = LearningRateFinder(model)\n",
        "  lrf.find(\n",
        "    aug.flow(X_train, Y_train, batch_size=batch_size),\n",
        "    1e-10, 1e+1,\n",
        "    stepsPerEpoch=np.ceil((len(X_train) / float(batch_size))),\n",
        "    batchSize=4)#batch_size)\n",
        "\n",
        "  # plot the loss for the various learning rates and save the\n",
        "  # resulting plot to disk\n",
        "  lrf.plot_loss()\n",
        "  #plt.savefig(lrfind_plot_path)\n",
        "\n",
        "  # gracefully exit the script so we can adjust our learning rates\n",
        "  # in the config and then train the network for our full set of\n",
        "  # epochs\n",
        "  print(\"[INFO] learning rate finder complete\")\n",
        "  print(\"[INFO] examine plot and adjust learning rates before training\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t01fLJ8_aNIT",
        "colab_type": "text"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArDz3c65V0jN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "outputId": "65f48844-a750-40cd-8bc5-8d16e38be66e"
      },
      "source": [
        "\"\"\"\n",
        "model.fit(X_train, Y_train,\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=True, \n",
        "          validation_data=(X_test, Y_test),\n",
        "          callbacks=callbacks)\n",
        "\"\"\"\n",
        "model.fit_generator(aug.flow(X_train, Y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True, seed=None),\n",
        "                    steps_per_epoch=len(X_train) // batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/48\n",
            "234/234 [==============================] - 24s 104ms/step - loss: 1.5116 - acc: 0.5226 - val_loss: 2.2002 - val_acc: 0.3596\n",
            "Epoch 2/48\n",
            "234/234 [==============================] - 23s 96ms/step - loss: 0.5636 - acc: 0.8047 - val_loss: 1.6328 - val_acc: 0.5328\n",
            "Epoch 3/48\n",
            "234/234 [==============================] - 23s 96ms/step - loss: 0.4429 - acc: 0.8446 - val_loss: 3.6661 - val_acc: 0.4430\n",
            "Epoch 4/48\n",
            "234/234 [==============================] - 23s 96ms/step - loss: 0.3964 - acc: 0.8614 - val_loss: 0.5986 - val_acc: 0.7966\n",
            "Epoch 5/48\n",
            "234/234 [==============================] - 23s 96ms/step - loss: 0.3649 - acc: 0.8704 - val_loss: 3.2382 - val_acc: 0.4521\n",
            "Epoch 6/48\n",
            "234/234 [==============================] - 23s 97ms/step - loss: 0.3465 - acc: 0.8769 - val_loss: 0.7076 - val_acc: 0.7281\n",
            "Epoch 7/48\n",
            "234/234 [==============================] - 23s 97ms/step - loss: 0.3283 - acc: 0.8847 - val_loss: 1.2489 - val_acc: 0.6594\n",
            "Epoch 8/48\n",
            "234/234 [==============================] - 23s 97ms/step - loss: 0.3191 - acc: 0.8855 - val_loss: 0.9817 - val_acc: 0.7166\n",
            "Epoch 9/48\n",
            "234/234 [==============================] - 23s 97ms/step - loss: 0.3011 - acc: 0.8935 - val_loss: 0.6718 - val_acc: 0.7857\n",
            "Epoch 10/48\n",
            "234/234 [==============================] - 22s 96ms/step - loss: 0.2826 - acc: 0.8995 - val_loss: 0.5716 - val_acc: 0.7931\n",
            "Epoch 11/48\n",
            "234/234 [==============================] - 23s 97ms/step - loss: 0.2641 - acc: 0.9060 - val_loss: 0.3097 - val_acc: 0.8967\n",
            "Epoch 12/48\n",
            "234/234 [==============================] - 22s 96ms/step - loss: 0.2506 - acc: 0.9101 - val_loss: 0.3255 - val_acc: 0.8891\n",
            "Epoch 13/48\n",
            "234/234 [==============================] - 22s 96ms/step - loss: 0.2377 - acc: 0.9151 - val_loss: 0.3502 - val_acc: 0.8809\n",
            "Epoch 14/48\n",
            "234/234 [==============================] - 23s 96ms/step - loss: 0.2278 - acc: 0.9184 - val_loss: 0.3410 - val_acc: 0.8798\n",
            "Epoch 15/48\n",
            "234/234 [==============================] - 23s 97ms/step - loss: 0.2137 - acc: 0.9249 - val_loss: 0.2510 - val_acc: 0.9144\n",
            "Epoch 16/48\n",
            "234/234 [==============================] - 23s 97ms/step - loss: 0.2025 - acc: 0.9285 - val_loss: 0.2374 - val_acc: 0.9170\n",
            "Epoch 17/48\n",
            "234/234 [==============================] - 23s 96ms/step - loss: 0.2001 - acc: 0.9290 - val_loss: 0.2493 - val_acc: 0.9134\n",
            "Epoch 18/48\n",
            "128/234 [===============>..............] - ETA: 9s - loss: 0.2034 - acc: 0.9281"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APY2Y9yveP18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_test, batch_size=batch_size)\n",
        "print(classification_report(y_true=Y_test.argmax(axis=1),\n",
        "                            y_pred=predictions.argmax(axis=1),\n",
        "                            target_names=Y_label))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}