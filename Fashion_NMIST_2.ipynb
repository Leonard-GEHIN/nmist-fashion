{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion NMIST 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yjKpaa3AbcJE",
        "guJzPQgabk3r"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaodwing/nmist-fashion/blob/Ajout-learning-rate-schedul/Fashion_NMIST_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycgJ9Q0OT0hF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Used Joseph Redmon on tiny darknet to produce my work\n",
        "#Used the work of Adrian Rosebrock for the learning rate schedule\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import utils\n",
        "from keras.callbacks import Callback, LearningRateScheduler\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.layers import Input, Dense, BatchNormalization, Conv2D, GlobalAveragePooling2D, Activation, LeakyReLU, MaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report\n",
        "from clr_callback import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcktjWXYbqD7",
        "colab_type": "text"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOTeN7Hzbpv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2019)\n",
        "tf.set_random_seed(2019)\n",
        "\n",
        "(X_train, Y_train ), (X_test, Y_test ) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IdADgEFvBnU",
        "colab_type": "text"
      },
      "source": [
        "#Tiny dark net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dUhDCpxu_pO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "num_classes = 10 \n",
        "epochs = 48\n",
        "img_cols = X_train.shape[1]\n",
        "img_rows = X_train.shape[2]\n",
        "\n",
        "# CLR hyper-parameters\n",
        "baseLr = 1e-7\n",
        "maxLr = 1e-2\n",
        "stepSize = 8\n",
        "clrMethod = \"triangular\"\n",
        "\n",
        "# name file\n",
        "lrfind_plot_path = os.path.sep.join([\"output\", \"lrfind_plot.png\"])\n",
        "training_plot_path = os.path.sep.join([\"output\", \"training_plot.png\"])\n",
        "clr_plot_path = os.path.sep.join([\"output\", \"clr_plot.png\"])\n",
        "\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols);\n",
        "  X_test  = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols);\n",
        "  input_shape = (1, img_rows, img_cols);\n",
        "else: \n",
        "  X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1);\n",
        "  X_test  = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1);\n",
        "  input_shape = (img_rows, img_cols, 1);\n",
        "\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test  = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test  /= 255\n",
        "\n",
        "Y_train = utils.to_categorical(Y_train, num_classes)\n",
        "Y_test  = utils.to_categorical(Y_test, num_classes)\n",
        "Y_label = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QBRL4_QRR4b",
        "colab_type": "text"
      },
      "source": [
        "#Class used for learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THdH4SCScc1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LearningRateDecay:\n",
        "\tdef plot(self, epochs, title=\"Learning Rate Schedule\"):\n",
        "\t\t# compute the set of learning rates for each corresponding\n",
        "\t\t# epoch\n",
        "\t\t#lrs = [self(i) for i in epochs]\n",
        "\t\ttempList = []\n",
        "\t\tepochs = np.arange(1,epochs)\n",
        "\t\tfor i in epochs:\n",
        "\t\t\ttempList.append(self(i))\n",
        "\t\tlrs = np.array(tempList)\n",
        "\n",
        "\t\t# the learning rate schedule\n",
        "\t\tplt.style.use(\"ggplot\")\n",
        "\t\tplt.figure()\n",
        "\t\tplt.plot(epochs, lrs)\n",
        "\t\tplt.title(title)\n",
        "\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\tplt.ylabel(\"Learning Rate\")\n",
        "\n",
        "\n",
        "class StepDecay(LearningRateDecay):\n",
        "\tdef __init__(self, initAlpha=0.01, factor=0.25, dropEvery=10):\n",
        "\t\t# store the base initial learning rate, drop factor, and\n",
        "\t\t# epochs to drop every\n",
        "\t\tself.initAlpha = initAlpha\n",
        "\t\tself.factor = factor\n",
        "\t\tself.dropEvery = dropEvery\n",
        "\t\n",
        "\tdef __call__(self, epoch):\n",
        "\t\t# compute the learning rate for the current epoch\n",
        "\t\texp = np.floor((1 + epoch) / self.dropEvery)\n",
        "\t\talpha = self.initAlpha * (self.factor ** exp)\n",
        "\t\t\n",
        "\t\t# return the learning rate\n",
        "\t\treturn float(alpha)\n",
        "\t\n",
        "\n",
        "class PolynomialDecay(LearningRateDecay):\n",
        "\tdef __init__(self, maxEpochs=100, initAlpha=0.01, power=1.0):\n",
        "\t\t# store the maximum number of epochs, base learning rate,\n",
        "\t\t# and power of the polynomial\n",
        "\t\t# power=1.0 = linear rate decay\n",
        "\t\tself.maxEpochs = maxEpochs\n",
        "\t\tself.initAlpha = initAlpha\n",
        "\t\tself.power = power\n",
        "\n",
        "\tdef __call__(self, epoch):\n",
        "\t\t# compute the new learning rate based on polynomial decay\n",
        "\t\tdecay = (1 - (epoch / float(self.maxEpochs))) ** self.power\n",
        "\t\talpha = self.initAlpha * decay\n",
        "\n",
        "\t\t# return the new learning rate\n",
        "\t\treturn float(alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcqhDcGwRa5T",
        "colab_type": "text"
      },
      "source": [
        "#Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbjuTgDpxN07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layerConv(output, filters, size, stride, pad, batch_normalization=True, activation='LeakyReLU'):\n",
        "  output = Conv2D(kernel_size = (size,size), filters = filters, strides=stride, padding=pad)(output)\n",
        "  if batch_normalization:\n",
        "    output = BatchNormalization()(output);\n",
        "  \n",
        "  #Activation layer\n",
        "  if activation=='LeakyReLU':\n",
        "    output = LeakyReLU(alpha = 0.1)(output)\n",
        "  else:\n",
        "    output = Activation(activation)(output)\n",
        "  \n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YunSxL6Rdby",
        "colab_type": "text"
      },
      "source": [
        "#Tensor model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp7tlDl2vysT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LAYERS\n",
        "#First set of convolution (9-12)\n",
        "input_img = Input(shape=input_shape)\n",
        "output = layerConv(output=input_img, filters=32, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=8 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=64, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=8 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=64, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "\n",
        "\"\"\"\n",
        "output = layerConv(output=input_img, filters=128, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=32 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=32 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "\n",
        "#Max pooling to get 14x14 feature (13)\n",
        "output = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(output)\n",
        "\n",
        "#Second set of convolution (14-19)\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output, filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #512\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output, filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #512\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #128\n",
        "\"\"\"\n",
        "\n",
        "#Last layer to get an output of 10 class (19+)\n",
        "output = layerConv(output=output, filters=10, size=1, batch_normalization=False, stride=1, pad=\"same\", activation='linear')\n",
        "output = GlobalAveragePooling2D()(output)\n",
        "output = Activation('softmax')(output)\n",
        "\n",
        "model = Model(input_img, output)\n",
        "#model.summary()\n",
        "\n",
        "\n",
        "#Learning Rate schedule\n",
        "LearningRateSchedule = CyclicLR( mode=clrMethod,\n",
        "                                base_lr=baseLr,\n",
        "                                max_lr=maxLr,\n",
        "                                step_size= stepSize * (X_train.shape[0] // batch_size))\n",
        "callbacks = []\n",
        "callbacks.append(LearningRateSchedule)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t01fLJ8_aNIT",
        "colab_type": "text"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArDz3c65V0jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = SGD(lr=baseLr, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train,\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=True, \n",
        "          validation_data=(X_test, Y_test),\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APY2Y9yveP18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_test, batch_size=batch_size)\n",
        "print(classification_report(y_true=Y_test.argmax(axis=1),\n",
        "                            y_pred=predictions.argmax(axis=1),\n",
        "                            target_names=Y_label))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}