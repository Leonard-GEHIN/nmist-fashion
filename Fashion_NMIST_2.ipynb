{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion NMIST 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "yjKpaa3AbcJE",
        "guJzPQgabk3r"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaodwing/nmist-fashion/blob/master/Fashion_NMIST_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycgJ9Q0OT0hF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from scipy import misc\n",
        "import tensorflow as tf\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "#MobileNet by keras\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.layers import Input,Dense,Dropout,Lambda\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "#MobileNetV2 by leonard\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "#Tiny  darknet\n",
        "from keras import utils\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers import BatchNormalization, Conv2D, GlobalAveragePooling2D, Activation, LeakyReLU, MaxPooling2D\n",
        "from keras.optimizers import SGD\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcktjWXYbqD7",
        "colab_type": "text"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOTeN7Hzbpv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2019)\n",
        "tf.set_random_seed(2019)\n",
        "\n",
        "(X_train, Y_train ), (X_test, Y_test ) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjKpaa3AbcJE",
        "colab_type": "text"
      },
      "source": [
        "#MobileNet de keras du mec qui a participé au fashion mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw3iIPXySMWa",
        "colab_type": "code",
        "outputId": "e012de14-f437-4410-f63b-d3de53e0035f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "\"\"\"\n",
        "#Credit\n",
        "#Code from 苏剑林, on https://kexue.fm/archives/4556\n",
        "height,width = 56,56\n",
        "\n",
        "\n",
        "input_image = Input(shape=(height,width))\n",
        "input_image_ = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,3),3,3))(input_image)\n",
        "base_model = MobileNet( input_shape=None,\n",
        "                        alpha=1.0,\n",
        "                        depth_multiplier=0.5,#1,\n",
        "                        dropout=1e-3,\n",
        "                        include_top=True,\n",
        "                        weights= None,#'imagenet',\n",
        "                        input_tensor=input_image_,\n",
        "                        pooling='avg',\n",
        "                        classes=1000)\n",
        "output = Dropout(0.5)(base_model.output)\n",
        "predict = Dense(10, activation='softmax')(output)\n",
        "\n",
        "model = Model(inputs=input_image, outputs=predict)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "X_train = X_train.reshape((-1,28,28))\n",
        "X_train = np.array([( np.resize(x, (height,width))).astype(float) for x in tqdm(iter(X_train))])/255.\n",
        "\n",
        "X_test = X_test.reshape((-1,28,28))\n",
        "X_test = np.array([( np.resize(x, (height,width))).astype(float) for x in tqdm(iter(X_test))])/255.\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=50, validation_data=(X_test, y_test))\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#Credit\\n#Code from 苏剑林, on https://kexue.fm/archives/4556\\nheight,width = 56,56\\n\\n\\ninput_image = Input(shape=(height,width))\\ninput_image_ = Lambda(lambda x: K.repeat_elements(K.expand_dims(x,3),3,3))(input_image)\\nbase_model = MobileNet( input_shape=None,\\n                        alpha=1.0,\\n                        depth_multiplier=0.5,#1,\\n                        dropout=1e-3,\\n                        include_top=True,\\n                        weights= None,#'imagenet',\\n                        input_tensor=input_image_,\\n                        pooling='avg',\\n                        classes=1000)\\noutput = Dropout(0.5)(base_model.output)\\npredict = Dense(10, activation='softmax')(output)\\n\\nmodel = Model(inputs=input_image, outputs=predict)\\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\\nmodel.summary()\\n\\nX_train = X_train.reshape((-1,28,28))\\nX_train = np.array([( np.resize(x, (height,width))).astype(float) for x in tqdm(iter(X_train))])/255.\\n\\nX_test = X_test.reshape((-1,28,28))\\nX_test = np.array([( np.resize(x, (height,width))).astype(float) for x in tqdm(iter(X_test))])/255.\\n\\nmodel.fit(X_train, y_train, batch_size=64, epochs=50, validation_data=(X_test, y_test))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guJzPQgabk3r",
        "colab_type": "text"
      },
      "source": [
        "#MobileNet de Leo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDbfoDEobkG8",
        "colab_type": "code",
        "outputId": "0f581c9e-84f0-4af4-807b-ee90b814a0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "\"\"\"\n",
        "batch_size = 128\n",
        "num_classes = 10 \n",
        "epochs = 12\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test  /= 255\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "score=model.evaluate(x_test, y_test,verbose=0)\n",
        "print('test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "model.save('Test_MNIST_MLP.h5')\n",
        "\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nbatch_size = 128\\nnum_classes = 10 \\nepochs = 12\\n\\n\\nx_train = x_train.astype('float32')\\nx_test  = x_test.astype('float32')\\nx_train /= 255\\nx_test  /= 255\\n\\n\\nmodel = Sequential()\\n\\n\\n\\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\\n\\nscore=model.evaluate(x_test, y_test,verbose=0)\\nprint('test loss:', score[0])\\nprint('Test accuracy:', score[1])\\n\\nmodel.save('Test_MNIST_MLP.h5')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IdADgEFvBnU",
        "colab_type": "text"
      },
      "source": [
        "#Tiny dark net Leo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dUhDCpxu_pO",
        "colab_type": "code",
        "outputId": "0459ff72-3bf8-474e-d6e3-7abaf17a1454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Used Joseph Redmon on tiny darknet to produce my work\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10 \n",
        "epochs = 40\n",
        "img_cols = X_train.shape[1]\n",
        "img_rows = X_train.shape[2]\n",
        "\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols);\n",
        "  X_test  = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols);\n",
        "  input_shape = (1, img_rows, img_cols);\n",
        "else: \n",
        "  X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1);\n",
        "  X_test  = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1);\n",
        "  input_shape = (img_rows, img_cols, 1);\n",
        "\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test  = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test  /= 255\n",
        "\n",
        "Y_train = utils.to_categorical(Y_train, num_classes)\n",
        "Y_test  = utils.to_categorical(Y_test, num_classes)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbjuTgDpxN07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function\n",
        "def layerConv(output, filters, size, stride, pad, batch_normalization=True, activation='LeakyReLU'):\n",
        "  output = Conv2D(kernel_size = (size,size), filters = filters, strides=stride, padding=pad)(output)\n",
        "  if batch_normalization:\n",
        "    output = BatchNormalization()(output);\n",
        "  \n",
        "  #Activation layer\n",
        "  if activation=='LeakyReLU':\n",
        "    output = LeakyReLU(alpha = 0.1)(output)\n",
        "  else:\n",
        "    output = Activation(activation)(output)\n",
        "  \n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp7tlDl2vysT",
        "colab_type": "code",
        "outputId": "870c4306-797a-4538-961c-a6f4d544e278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LAYERS\n",
        "#First set of convolution (9-12)\n",
        "input_img = Input(shape=input_shape)\n",
        "output = layerConv(output=input_img, filters=128, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=32 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=32 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "\n",
        "#Max pooling to get 14x14 feature (13)\n",
        "output = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(output)\n",
        "\n",
        "#Second set of convolution (14-19)\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output, filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #512\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output, filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #512\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #128\n",
        "\n",
        "#Last layer to get an output of 10 class (19+)\n",
        "output = layerConv(output=output, filters=10, size=1, batch_normalization=False, stride=1, pad=\"same\", activation='linear')\n",
        "output = GlobalAveragePooling2D()(output)\n",
        "output = Activation('softmax')(output)\n",
        "#output = cost layer\n",
        "\n",
        "model = Model(input_img, output)\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "opt = SGD(lr=1e-2, momentum=0.9, decay=1e-2/epochs)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train,\n",
        "          epochs=epochs,\n",
        "          batch_size=256,\n",
        "          shuffle=True, \n",
        "          validation_data=(X_test, Y_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 28, 28, 128)       1280      \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_46 (LeakyReLU)   (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 28, 28, 32)        4128      \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_47 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 28, 28, 256)       73984     \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_48 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 28, 28, 32)        8224      \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 28, 28, 256)       73984     \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 28, 28, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 14, 14, 64)        16448     \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 14, 14, 256)       147712    \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)   (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 14, 14, 64)        16448     \n",
            "_________________________________________________________________\n",
            "batch_normalization_54 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 14, 14, 256)       147712    \n",
            "_________________________________________________________________\n",
            "batch_normalization_55 (Batc (None, 14, 14, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_54 (LeakyReLU)   (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 14, 14, 64)        16448     \n",
            "_________________________________________________________________\n",
            "batch_normalization_56 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_55 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 14, 14, 10)        650       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 14, 14, 10)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_5 ( (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 512,650\n",
            "Trainable params: 509,834\n",
            "Non-trainable params: 2,816\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 51s 846us/step - loss: 0.7709 - acc: 0.7563 - val_loss: 1.1189 - val_acc: 0.5850\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.3665 - acc: 0.8765 - val_loss: 0.4824 - val_acc: 0.8303\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 48s 801us/step - loss: 0.2935 - acc: 0.8999 - val_loss: 0.5037 - val_acc: 0.8200\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 48s 800us/step - loss: 0.2570 - acc: 0.9119 - val_loss: 0.4401 - val_acc: 0.8404\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 48s 800us/step - loss: 0.2298 - acc: 0.9209 - val_loss: 0.3285 - val_acc: 0.8885\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.2110 - acc: 0.9275 - val_loss: 0.4365 - val_acc: 0.8559\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 48s 800us/step - loss: 0.1945 - acc: 0.9329 - val_loss: 0.3635 - val_acc: 0.8678\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.1755 - acc: 0.9407 - val_loss: 0.2678 - val_acc: 0.9064\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.1652 - acc: 0.9443 - val_loss: 0.2901 - val_acc: 0.8987\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 48s 801us/step - loss: 0.1508 - acc: 0.9496 - val_loss: 0.3746 - val_acc: 0.8703\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.1381 - acc: 0.9546 - val_loss: 0.4092 - val_acc: 0.8661\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 48s 798us/step - loss: 0.1289 - acc: 0.9577 - val_loss: 0.3140 - val_acc: 0.8893\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 48s 798us/step - loss: 0.1179 - acc: 0.9622 - val_loss: 0.3532 - val_acc: 0.8852\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 48s 797us/step - loss: 0.1075 - acc: 0.9661 - val_loss: 0.2896 - val_acc: 0.8997\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.0973 - acc: 0.9706 - val_loss: 0.3142 - val_acc: 0.8928\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.0883 - acc: 0.9737 - val_loss: 0.2876 - val_acc: 0.9073\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 48s 798us/step - loss: 0.0784 - acc: 0.9782 - val_loss: 0.2410 - val_acc: 0.9199\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 48s 797us/step - loss: 0.0710 - acc: 0.9804 - val_loss: 0.2763 - val_acc: 0.9081\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.0624 - acc: 0.9840 - val_loss: 0.5064 - val_acc: 0.8579\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 48s 797us/step - loss: 0.0543 - acc: 0.9873 - val_loss: 0.3237 - val_acc: 0.9018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f37758f2668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}