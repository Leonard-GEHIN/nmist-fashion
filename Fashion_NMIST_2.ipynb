{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion NMIST 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yjKpaa3AbcJE",
        "guJzPQgabk3r"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaodwing/nmist-fashion/blob/Ajout-learning-rate-schedul/Fashion_NMIST_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycgJ9Q0OT0hF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Used Joseph Redmon on tiny darknet to produce my work\n",
        "#Used the work of Adrian Rosebrock for the learning rate schedule\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import utils\n",
        "from keras.callbacks import Callback, LearningRateScheduler\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.layers import Input, Dense, BatchNormalization, Conv2D, GlobalAveragePooling2D, Activation, LeakyReLU, MaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report\n",
        "from clr_callback import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcktjWXYbqD7",
        "colab_type": "text"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOTeN7Hzbpv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2019)\n",
        "tf.set_random_seed(2019)\n",
        "\n",
        "(X_train, Y_train ), (X_test, Y_test ) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IdADgEFvBnU",
        "colab_type": "text"
      },
      "source": [
        "#Tiny dark net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dUhDCpxu_pO",
        "colab_type": "code",
        "outputId": "cde8c580-3db7-4ea3-df5d-badb1bf5416b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "batch_size = 256\n",
        "num_classes = 10 \n",
        "epochs = 10\n",
        "img_cols = X_train.shape[1]\n",
        "img_rows = X_train.shape[2]\n",
        "\n",
        "# CLR hyper-parameters\n",
        "baseLr = 1e-7\n",
        "maxLr = 1e-2\n",
        "stepSize = 8\n",
        "clrMethod = \"triangular\"\n",
        "\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols);\n",
        "  X_test  = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols);\n",
        "  input_shape = (1, img_rows, img_cols);\n",
        "else: \n",
        "  X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1);\n",
        "  X_test  = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1);\n",
        "  input_shape = (img_rows, img_cols, 1);\n",
        "\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test  = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test  /= 255\n",
        "\n",
        "Y_train = utils.to_categorical(Y_train, num_classes)\n",
        "Y_test  = utils.to_categorical(Y_test, num_classes)\n",
        "Y_label = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QBRL4_QRR4b",
        "colab_type": "text"
      },
      "source": [
        "#Class used for learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THdH4SCScc1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LearningRateDecay:\n",
        "\tdef plot(self, epochs, title=\"Learning Rate Schedule\"):\n",
        "\t\t# compute the set of learning rates for each corresponding\n",
        "\t\t# epoch\n",
        "\t\t#lrs = [self(i) for i in epochs]\n",
        "\t\ttempList = []\n",
        "\t\tepochs = np.arange(1,epochs)\n",
        "\t\tfor i in epochs:\n",
        "\t\t\ttempList.append(self(i))\n",
        "\t\tlrs = np.array(tempList)\n",
        "\n",
        "\t\t# the learning rate schedule\n",
        "\t\tplt.style.use(\"ggplot\")\n",
        "\t\tplt.figure()\n",
        "\t\tplt.plot(epochs, lrs)\n",
        "\t\tplt.title(title)\n",
        "\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\tplt.ylabel(\"Learning Rate\")\n",
        "\n",
        "\n",
        "class StepDecay(LearningRateDecay):\n",
        "\tdef __init__(self, initAlpha=0.01, factor=0.25, dropEvery=10):\n",
        "\t\t# store the base initial learning rate, drop factor, and\n",
        "\t\t# epochs to drop every\n",
        "\t\tself.initAlpha = initAlpha\n",
        "\t\tself.factor = factor\n",
        "\t\tself.dropEvery = dropEvery\n",
        "\t\n",
        "\tdef __call__(self, epoch):\n",
        "\t\t# compute the learning rate for the current epoch\n",
        "\t\texp = np.floor((1 + epoch) / self.dropEvery)\n",
        "\t\talpha = self.initAlpha * (self.factor ** exp)\n",
        "\t\t\n",
        "\t\t# return the learning rate\n",
        "\t\treturn float(alpha)\n",
        "\t\n",
        "\n",
        "class PolynomialDecay(LearningRateDecay):\n",
        "\tdef __init__(self, maxEpochs=100, initAlpha=0.01, power=1.0):\n",
        "\t\t# store the maximum number of epochs, base learning rate,\n",
        "\t\t# and power of the polynomial\n",
        "\t\t# power=1.0 = linear rate decay\n",
        "\t\tself.maxEpochs = maxEpochs\n",
        "\t\tself.initAlpha = initAlpha\n",
        "\t\tself.power = power\n",
        "\n",
        "\tdef __call__(self, epoch):\n",
        "\t\t# compute the new learning rate based on polynomial decay\n",
        "\t\tdecay = (1 - (epoch / float(self.maxEpochs))) ** self.power\n",
        "\t\talpha = self.initAlpha * decay\n",
        "\n",
        "\t\t# return the new learning rate\n",
        "\t\treturn float(alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcqhDcGwRa5T",
        "colab_type": "text"
      },
      "source": [
        "#Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbjuTgDpxN07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layerConv(output, filters, size, stride, pad, batch_normalization=True, activation='LeakyReLU'):\n",
        "  output = Conv2D(kernel_size = (size,size), filters = filters, strides=stride, padding=pad)(output)\n",
        "  if batch_normalization:\n",
        "    output = BatchNormalization()(output);\n",
        "  \n",
        "  #Activation layer\n",
        "  if activation=='LeakyReLU':\n",
        "    output = LeakyReLU(alpha = 0.1)(output)\n",
        "  else:\n",
        "    output = Activation(activation)(output)\n",
        "  \n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YunSxL6Rdby",
        "colab_type": "text"
      },
      "source": [
        "#Tensor model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp7tlDl2vysT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LAYERS\n",
        "#First set of convolution (9-12)\n",
        "input_img = Input(shape=input_shape)\n",
        "output = layerConv(output=input_img, filters=32, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=8 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=64, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=8 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=64, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "\n",
        "\"\"\"\n",
        "output = layerConv(output=input_img, filters=128, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=32 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=32 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output   , filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "\n",
        "#Max pooling to get 14x14 feature (13)\n",
        "output = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(output)\n",
        "\n",
        "#Second set of convolution (14-19)\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output, filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #512\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU')\n",
        "output = layerConv(output=output, filters=256, size=3, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #512\n",
        "output = layerConv(output=output, filters=64 , size=1, batch_normalization=True, stride=1, pad=\"same\", activation='LeakyReLU') #128\n",
        "\"\"\"\n",
        "\n",
        "#Last layer to get an output of 10 class (19+)\n",
        "output = layerConv(output=output, filters=10, size=1, batch_normalization=False, stride=1, pad=\"same\", activation='linear')\n",
        "output = GlobalAveragePooling2D()(output)\n",
        "output = Activation('softmax')(output)\n",
        "\n",
        "model = Model(input_img, output)\n",
        "#model.summary()\n",
        "\n",
        "\n",
        "#Learning Rate schedule\n",
        "LearningRateSchedule = CyclicLR( mode=clrMethod,\n",
        "                                base_lr=baseLr,\n",
        "                                max_lr=maxLr,\n",
        "                                step_size= stepSize * (X_train.shape[0] // batch_size))\n",
        "callbacks = []\n",
        "callbacks.append(LearningRateSchedule)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t01fLJ8_aNIT",
        "colab_type": "text"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArDz3c65V0jN",
        "colab_type": "code",
        "outputId": "b84b0d7e-0069-4480-eb2f-8df57101f60f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "opt = SGD(lr=baseLr, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train,\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=True, \n",
        "          validation_data=(X_test, Y_test),\n",
        "          callbacks=callbacks)\n",
        "\"\"\"\n",
        "#test\n",
        "aug = ImageDataGenerator(width_shift_range=0.1,\n",
        "\theight_shift_range=0.1, horizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n",
        "model.fit_generator(\n",
        "\taug.flow(X_train, Y_train, batch_size=batch_size),\n",
        "\tvalidation_data=(X_test, Y_test),\n",
        "\tsteps_per_epoch=X_train.shape[0] // batch_size,\n",
        "\tepochs=epochs,\n",
        "\tcallbacks=callbacks,\n",
        "\tverbose=1)\n",
        "\"\"\""
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 19s 309us/step - loss: 2.1464 - acc: 0.2492 - val_loss: 1.7915 - val_acc: 0.4391\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 1.5717 - acc: 0.5052 - val_loss: 1.6053 - val_acc: 0.3552\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 1.2299 - acc: 0.6494 - val_loss: 1.5049 - val_acc: 0.4027\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.9697 - acc: 0.7183 - val_loss: 1.1868 - val_acc: 0.5382\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.7907 - acc: 0.7524 - val_loss: 0.8022 - val_acc: 0.7228\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.6790 - acc: 0.7739 - val_loss: 1.3262 - val_acc: 0.5752\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.6098 - acc: 0.7925 - val_loss: 0.9495 - val_acc: 0.6580\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 17s 291us/step - loss: 0.5596 - acc: 0.8090 - val_loss: 0.6966 - val_acc: 0.7375\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 18s 292us/step - loss: 0.5158 - acc: 0.8238 - val_loss: 0.6145 - val_acc: 0.7754\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 18s 293us/step - loss: 0.4788 - acc: 0.8357 - val_loss: 0.7052 - val_acc: 0.7616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#test\\naug = ImageDataGenerator(width_shift_range=0.1,\\n\\theight_shift_range=0.1, horizontal_flip=True,\\n\\tfill_mode=\"nearest\")\\nmodel.fit_generator(\\n\\taug.flow(X_train, Y_train, batch_size=batch_size),\\n\\tvalidation_data=(X_test, Y_test),\\n\\tsteps_per_epoch=X_train.shape[0] // batch_size,\\n\\tepochs=epochs,\\n\\tcallbacks=callbacks,\\n\\tverbose=1)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APY2Y9yveP18",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "5a535ad3-d65e-4c35-acc5-ae161d4eb050"
      },
      "source": [
        "predictions = model.predict(X_test, batch_size=batch_size)\n",
        "print(classification_report(y_true=Y_test.argmax(axis=1),\n",
        "                            y_pred=predictions.argmax(axis=1),\n",
        "                            target_names=Y_label))\n",
        "\n",
        "# plot the learning rate history\n",
        "N = np.arange(0, len(clr.history[\"lr\"]))\n",
        "plt.figure()\n",
        "plt.plot(N, clr.history[\"lr\"])\n",
        "plt.title(\"Cyclical Learning Rate (CLR)\")\n",
        "plt.xlabel(\"Training Iterations\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top       0.55      0.94      0.69      1000\n",
            "     Trouser       0.89      0.96      0.92      1000\n",
            "    Pullover       0.59      0.85      0.70      1000\n",
            "       Dress       0.79      0.71      0.75      1000\n",
            "        Coat       0.92      0.27      0.42      1000\n",
            "      Sandal       1.00      0.81      0.89      1000\n",
            "       Shirt       0.57      0.35      0.43      1000\n",
            "     Sneaker       0.93      0.81      0.86      1000\n",
            "         Bag       0.96      0.93      0.94      1000\n",
            "  Ankle boot       0.77      0.98      0.86      1000\n",
            "\n",
            "    accuracy                           0.76     10000\n",
            "   macro avg       0.80      0.76      0.75     10000\n",
            "weighted avg       0.80      0.76      0.75     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-8daec7bb17ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# plot the learning rate history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clr' is not defined"
          ]
        }
      ]
    }
  ]
}